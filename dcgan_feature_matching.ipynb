{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36964bit24bb1e34045442d8937ff01089d70b12",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed for Recreation of Results. Determining Randomness\n",
    "random_seed = random.randint(0,10000)\n",
    "print(\"Random Seed : \", random_seed)\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arguments \n",
    "data = '/home/shubhansh/Work/Datasets/celebA/img_align_celeba'\n",
    "workers = 4\n",
    "batch_size = 128\n",
    "img_size = 64\n",
    "n_channel = 3\n",
    "n_z = 100\n",
    "n_fpg = 64\n",
    "n_fpd = 64\n",
    "n_epochs = 5\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "ngpu = 1\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Data Loader with resize,crop and Normalization\n",
    "dataset  = dset.ImageFolder(root = data,transform = transforms.Compose([transforms.Resize(img_size),transforms.CenterCrop(img_size),transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))]))\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset,batch_size=batch_size,shuffle=True,num_workers=workers)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample Batch for Visualization\n",
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64],padding=2,normalize=True).cpu(),(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight Initialization for All networks(as per paper)\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv')!=-1:\n",
    "        nn.init.normal_(m.weight.data,0.0,0.02)\n",
    "    elif classname.find('BatchNorm')!=-1:\n",
    "        nn.init.normal_(m.weight.data,1.0,0.02)\n",
    "        nn.init.constant_(m.bias.data,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generator Architecture\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator,self).__init__()\n",
    "        self.convt1 = nn.ConvTranspose2d(n_z,n_fpg*8,4,1,0,bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(n_fpg*8)\n",
    "\n",
    "        self.convt2 = nn.ConvTranspose2d(n_fpg*8,n_fpg*4,4,2,1,bias = False)\n",
    "        self.bn2 = nn.BatchNorm2d(n_fpg*4)\n",
    "\n",
    "        self.convt3 = nn.ConvTranspose2d(n_fpg*4,n_fpg*2,4,2,1,bias = False)\n",
    "        self.bn3 = nn.BatchNorm2d(n_fpg*2)\n",
    "\n",
    "        self.convt4 = nn.ConvTranspose2d(n_fpg*2,n_fpg,4,2,1,bias = False)\n",
    "        self.bn4 = nn.BatchNorm2d(n_fpg)\n",
    "\n",
    "        self.convt5 = nn.ConvTranspose2d( n_fpg, n_channel, 4, 2, 1, bias=False)\n",
    "\n",
    "    \n",
    "    def forward(self,input):\n",
    "\n",
    "        o1_1 = self.convt1(input)\n",
    "        o1_2 = self.bn1(o1_1)\n",
    "        o1_3 = F.relu(o1_2)\n",
    "\n",
    "        o2_1 = self.convt2(o1_3)\n",
    "        o2_2 = self.bn2(o2_1)\n",
    "        o2_3 = F.relu(o2_2)\n",
    "\n",
    "        o3_1 = self.convt3(o2_3)\n",
    "        o3_2 = self.bn3(o3_1)\n",
    "        o3_3 = F.relu(o3_2)\n",
    "\n",
    "        o4_1 = self.convt4(o3_3)\n",
    "        o4_2 = self.bn4(o4_1)\n",
    "        o4_3 = F.relu(o4_2)\n",
    "\n",
    "        output = F.tanh(self.convt5(o4_3))\n",
    "\n",
    "        return output\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator to GPU\n",
    "netG = Generator().to(device)\n",
    "# Initialization of Weights of Generator\n",
    "netG.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Discriminator Architecture\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(n_channel,n_fpd,4,2,1,bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(n_fpd)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(n_fpd,n_fpd*2,4,2,1,bias = False)\n",
    "        self.bn2 = nn.BatchNorm2d(n_fpd*2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(n_fpd*2,n_fpd*4,4,2,1,bias = False)\n",
    "        self.bn3 = nn.BatchNorm2d(n_fpd*4)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(n_fpd*4,n_fpd*8,4,2,1,bias = False)\n",
    "        self.bn4 = nn.BatchNorm2d(n_fpg*8)\n",
    "\n",
    "        self.conv5 = nn.Conv2d( n_fpd*8, 1, 4, 1, 0, bias=False)\n",
    "    \n",
    "    def forward(self,input):\n",
    "\n",
    "        o1_1 = self.conv1(input)\n",
    "        o1_2 = self.bn1(o1_1)\n",
    "        o1_3 = F.leaky_relu(o1_2,0.2)\n",
    "\n",
    "        o2_1 = self.conv2(o1_3)\n",
    "        o2_2 = self.bn2(o2_1)\n",
    "        o2_3 = F.leaky_relu(o2_2,0.2)\n",
    "\n",
    "        o3_1 = self.conv3(o2_3)\n",
    "        o3_2 = self.bn3(o3_1)\n",
    "        o3_3 = F.leaky_relu(o3_2,0.2)\n",
    "\n",
    "        o4_1 = self.conv4(o3_3)\n",
    "        o4_2 = self.bn4(o4_1)\n",
    "        o4_3 = F.leaky_relu(o4_2,0.2)\n",
    "\n",
    "        output = F.sigmoid(self.conv5(o4_3))\n",
    "\n",
    "        return output\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator to GPU\n",
    "netD = Discriminator().to(device)\n",
    "# Discriminator Weight Initialization\n",
    "netD.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Binary Cross Entropy Loss\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "##For Results\n",
    "fixed_batch = torch.randn(64,n_z,1,1,device=device)\n",
    "## Assign 1 to Real Images\n",
    "real_label = 1\n",
    "## Assign 0 to Fake Images\n",
    "fake_label = 0\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Training .....\")\n",
    "for epoch in range(n_epochs):\n",
    "    for i,data in enumerate(dataloader,0):\n",
    "\n",
    "        ### Discriminator Update\n",
    "\n",
    "        ## Making Zero Gradient in Start(Remove residual gradient from previous epoch)\n",
    "        netD.zero_grad()\n",
    "\n",
    "\n",
    "        ## Passing Real Batch\n",
    "        real_data = data[0].to(device)\n",
    "        b_size = real_data.shape[0]\n",
    "        label = torch.full((b_size,),real_label,device=device)\n",
    "        output = netD(real_data).view(-1)\n",
    "        netD_error_real = criterion(output,label)\n",
    "        ##compute gradient\n",
    "        netD_error_real.backward()\n",
    "        ## Average output as it will tell how good out Discriminator is. This value should approach to 1.\n",
    "        D_real = output.mean().item()\n",
    "\n",
    "        ## Generate fake images using generator\n",
    "\n",
    "        noise = torch.randn(b_size,n_z,1,1,device= device)\n",
    "        fake_data = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "\n",
    "        ## Passing Fake Batches\n",
    "\n",
    "        output = netD(fake_data.detach()).view(-1)\n",
    "        netD_error_fake = criterion(output,label)\n",
    "        ##compute gradient (Gradient is additive so network have aggregated gradient values)\n",
    "        netD_error_fake.backward()\n",
    "        ## Average output as it will tell how good out Discriminator is. This value should approach to 0.\n",
    "        D_fake = output.mean().item()\n",
    "\n",
    "        ## Total Discriminator Error\n",
    "        netD_error = netD_error_fake + netD_error_real\n",
    "\n",
    "        ## Update the Discriminator\n",
    "        optimizerD.step()\n",
    "\n",
    "\n",
    "        ### Generator Update\n",
    "        \n",
    "        ## Making Zero Gradient in Start(Remove residual gradient from previous epoch)\n",
    "        netG.zero_grad()\n",
    "\n",
    "        output = netD(fake_data).view(-1)\n",
    "        ## Generator want label to be 1, so error will be considered when discriminator will assign  generated images a non zero value.\n",
    "        label.fill_(real_label)\n",
    "        netG_error = criterion(output,label)\n",
    "        netG_error.backward()\n",
    "        DG_fake = output.mean().item()\n",
    "\n",
    "        ## Update the Generator\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i%50==0:\n",
    "            print(\"[%d/%d],[%d/%d]\\t Loss_D: %0.4f\\t Loss_G: %0.4f\\t Avg_D_real: %0.4f\\t Avg_D/DG: [%0.4f/%0.4f]\" % (epoch,n_epochs,i,len(dataloader),netD_error.item(),netG_error.item(),D_real,D_fake,DG_fake))\n",
    "\n",
    "\n",
    "        G_losses.append(netG_error.item())\n",
    "        D_losses.append(netD_error.item())\n",
    "\n",
    "        if(iters%500==0) or ((epoch==n_epochs-1)and(i==len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake_img = netG(fixed_batch).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake_img,padding=2,normalize=True))\n",
    "\n",
    "        iters+=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ""
   ]
  }
 ]
}